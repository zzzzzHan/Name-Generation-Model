{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab():\n",
    "    # Construct the character 'vocabulary'\n",
    "    # we allow lowercase, uppercase and digits only, along with special characters:\n",
    "    # \"\" - Empty string used to denote elements for the RNN to ignore\n",
    "    # \"<bos>\" - Beginning of sequence token for the input the the RNN\n",
    "    # \".\" - End of sequence token\n",
    "    vocab = [\"\", \"<bos>\", \".\"] + list(string.ascii_lowercase + string.ascii_uppercase + string.digits + \" \")\n",
    "    id_to_char = {i: v for i, v in enumerate(vocab)} # maps from ids to characters\n",
    "    char_to_id = {v: i for i, v in enumerate(vocab)} # maps from characters to ids\n",
    "    return vocab, id_to_char, char_to_id\n",
    "\n",
    "def load_data(filename):\n",
    "    # read in the list of names\n",
    "    data = json.load(open(filename, \"r\"))\n",
    "    # append the end of sequence token to each name\n",
    "    data = [v+'.' for v in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"names_small.json\")\n",
    "\n",
    "# get the letter 'vocabulary'\n",
    "vocab, id_to_char, char_to_id = get_vocab()\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 22, 7, 24, 7, 16, 65, 47, 10, 3, 16, 7, 65, 41, 5, 32, 17, 16, 3, 14, 0, 0, 0, 0, 0]\n",
      "Steven Shane McDonald.\n"
     ]
    }
   ],
   "source": [
    "print([char_to_id[c] for c in data[11][:20]]+5*[0])\n",
    "print(data[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqs_to_ids(seqs, char_to_id, max_len=20):\n",
    "    \"\"\"Takes a list of names and turns them into a list of tokens ids.\n",
    "    Responsible for padding sequences shorter than max_len with 0 so that all sequences are max_len.\n",
    "    Also truncates names that are longer than max_len.\n",
    "    Should also skip empty sequences if there are any.\n",
    "\n",
    "    Args:\n",
    "        seqs (list(str)): A list of names as strings.\n",
    "        char_to_id (dict(str : int)): The mapping for characters to token ids\n",
    "        max_len (int, optional): The maximum length of the ouput sequence. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "        np.array: the names represented using token ids as 2d numpy array, \n",
    "            where each row corresponds to a name. The size of the array should be N * max_len\n",
    "            where N is the number of non-empty sequences input. Padded with zeros if needed.\n",
    "    \"\"\"\n",
    "    all_seqs = []\n",
    "    # TODO: implement this function to turn a list of names into a 2d padded array of token ids\n",
    "    for name in seqs:\n",
    "        truncated = name[:max_len]\n",
    "        name_sequence = [char_to_id[c] for c in truncated]\n",
    "        if (len(name_sequence))<max_len:\n",
    "            name_sequence=name_sequence+(max_len-len(name_sequence))*[0]\n",
    "        all_seqs.append(name_sequence)    \n",
    "    return np.array(all_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = seqs_to_ids(data, char_to_id)\n",
    "X = np.concatenate([np.ones((Y.shape[0], 1)), Y[:, :-1]], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.9\n",
    "num_train = int(X.shape[0]*train_frac)\n",
    "\n",
    "Xtrain = torch.tensor(X[:num_train], dtype=torch.long)\n",
    "Ytrain = torch.tensor(Y[:num_train], dtype=torch.long)\n",
    "Xval = torch.tensor(X[num_train:], dtype=torch.long)\n",
    "Yval = torch.tensor(Y[num_train:], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size = 32, gru_size=32):\n",
    "        super(RNNLM, self).__init__()\n",
    "\n",
    "        # store layer sizes\n",
    "        self.emb_size = emb_size\n",
    "        self.gru_size = gru_size\n",
    "\n",
    "        # for embedding characters (ignores those with value 0: the padded values)\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(emb_size, gru_size, batch_first=True)\n",
    "        # linear layer for output\n",
    "        self.linear = nn.Linear(gru_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, h_last=None):\n",
    "        \"\"\"Takes a batch of names/sequences expressed as token ids and passes them through the GRU.\n",
    "            The output is the predicted (un-normalized) probabilities of \n",
    "            the next token for all prefixes of the input sequences.\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): A 2d tensor of longs giving the token ids for each batch. Shape B * S\n",
    "                where B is the batch size (any batch size >= 1 is permitted), S is the length of the sequence.\n",
    "            h_last (torch.tensor, optional): A 2d float tensor of size B * G where B is the batch size and G\n",
    "                is the dimensionality of the GRU hidden state. The hidden state from the previous step, provide only if \n",
    "                generating sequences iteratively. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            tuple(torch.tensor, torch.tensor): first element of the tuple is the B * S * V where V is the vocabulary size.\n",
    "                This is the logit output of the RNNLM. The second element is the hidden state of the final step\n",
    "                of the GRU it should be B * G dimensional.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: implement this function which does the forward pass of the RNNLM network\n",
    "        embedded = self.emb(x)\n",
    "        output, h = self.gru(embedded,h_last)\n",
    "        #out = self.linear(output[:,-1,:])\n",
    "        out = self.linear(output)\n",
    "        #out = torch.nn.functional.softmax(out,dim=1)\n",
    "        return out, h[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, Xtrain, Ytrain, Xval, Yval, id_to_char, max_epoch):\n",
    "    \"\"\"Train the RNNLM model using the Xtrain and Ytrain examples.\n",
    "    Uses batch stochastic gradient descent with the Adam optimizer on \n",
    "    the mean cross entropy loss. Prints out the validation loss\n",
    "    after each epoch using calc_val_loss.\n",
    "\n",
    "    Args:\n",
    "        model (RNNLM): the RNNLM model.\n",
    "        Xtrain (torch.tensor): The training data input sequence of size Nt * S. \n",
    "            Nt is the number of training examples, S is the sequence length. \n",
    "            The sequences always start with the <bos> token id.\n",
    "            The rest of the sequence is just Ytrain shifted to the right one position.\n",
    "            The sequence is zero padded.\n",
    "        Ytrain (torch.tensor): The expected output sequence of size Nt * S. \n",
    "            Does not start with the <bos> token.\n",
    "        Xval (torch.tensor): The validation data input sequence of size Nv * S. \n",
    "            Nv is the number of validation examples, S is the sequence length. \n",
    "            The sequences always start with the <bos> token id.\n",
    "            The rest of sequence is just Yval shifted to the right one position.\n",
    "            The sequence is zero padded.\n",
    "        Yval (torch.tensor): The expected output sequence for the validation data of size Nv * S. \n",
    "            Does not start with the <bos> token. Is zero padded.\n",
    "        id_to_char (dict(int : str)): A mapping from ids to tokens.\n",
    "        max_epoch (int): the maximum number of epochs to train for.\n",
    "    \"\"\"\n",
    "    # construct the adam optimizer\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    # construct the cross-entropy loss function\n",
    "    # we want to ignore padding cells with value == 0\n",
    "    lossfn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    # calculate number of batches\n",
    "    batch_size = 32\n",
    "    num_batches = int(Xtrain.shape[0] / batch_size)\n",
    "    average_loss = 0 \n",
    "    # run the main training loop over many epochs\n",
    "    for e in range(max_epoch):\n",
    "        total_loss = 0\n",
    "        total_chars = 0\n",
    "        for t in range(0,num_batches-1):\n",
    "            batch_X = get_batch(t,Xtrain,batch_size)\n",
    "            batch_Y = get_batch(t,Ytrain,batch_size)\n",
    "            model.zero_grad()\n",
    "            out, h = model(batch_X)\n",
    "            #out= torch.nn.functional.softmax(out,dim=1)\n",
    "            loss=lossfn(out.permute(0, 2, 1), batch_Y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        val_loss = calc_val_loss(model, Xval, Yval)\n",
    "        print(val_loss)\n",
    "        # TODO: implement the training loop of the RNNLM model\n",
    "def get_batch(t,data,batch_size):\n",
    "    return data[t*batch_size:(t+1)*batch_size]        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     for n in range(num_batches):\n",
    "\n",
    "#         # calculate batch start end idxs \n",
    "#         s = n * batch_size\n",
    "#         e = (n+1)*batch_size\n",
    "#         if e > Xval.shape[0]:\n",
    "#             e = Xval.shape[0]\n",
    "\n",
    "#         # compute output of model        \n",
    "#         out,_ = model(Xval[s:e])\n",
    "\n",
    "#         # compute loss and store\n",
    "#         loss = lossfn(out.permute(0, 2, 1), Yval[s:e]).detach().cpu().numpy()\n",
    "#         total_loss += loss\n",
    "\n",
    "#         char_count = torch.count_nonzero(Yval[s:e].flatten())\n",
    "#         total_chars += char_count.detach().cpu().numpy()\n",
    "\n",
    "#     # compute average loss per character\n",
    "#     total_loss /= total_chars\n",
    "    \n",
    "#     # set the model back to training mode in case we need gradients later\n",
    "#     model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_val_loss(model, Xval, Yval):\n",
    "    \"\"\"Calculates the validation loss in average nats per character.\n",
    "\n",
    "    Args:\n",
    "        model (RNNLM): the RNNLM model.\n",
    "        Xval (torch.tensor): The validation data input sequence of size B * S. \n",
    "            B is the batch size, S is the sequence length. The sequences always start with the <bos> token id.\n",
    "            The rest of sequence is just Yval shifted to the right one position.\n",
    "            The sequence is zero padded.\n",
    "        Yval (torch.tensor): The expected output sequence for the validation data of size B * S. \n",
    "            Does not start with the <bos> token. Is zero padded.\n",
    "\n",
    "    Returns:\n",
    "        float: validation loss in average nats per character.\n",
    "    \"\"\"\n",
    "\n",
    "    # use cross entropy loss\n",
    "    lossfn = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "\n",
    "    # put the model into eval mode because we don't need gradients\n",
    "    model.eval()\n",
    "\n",
    "    # calculate number of batches, we need to be precise this time\n",
    "    batch_size = 32\n",
    "    num_batches = int(Xval.shape[0] / batch_size)\n",
    "    if Xval.shape[0] % batch_size != 0:\n",
    "        num_batches += 1\n",
    "\n",
    "    # sum up the total loss\n",
    "    total_loss = 0\n",
    "    total_chars = 0\n",
    "    for n in range(num_batches):\n",
    "\n",
    "        # calculate batch start end idxs \n",
    "        s = n * batch_size\n",
    "        e = (n+1)*batch_size\n",
    "        if e > Xval.shape[0]:\n",
    "            e = Xval.shape[0]\n",
    "\n",
    "        # compute output of model        \n",
    "        out,_ = model(Xval[s:e])\n",
    "\n",
    "        # compute loss and store\n",
    "        loss = lossfn(out.permute(0, 2, 1), Yval[s:e]).detach().cpu().numpy()\n",
    "        total_loss += loss\n",
    "\n",
    "        char_count = torch.count_nonzero(Yval[s:e].flatten())\n",
    "        total_chars += char_count.detach().cpu().numpy()\n",
    "\n",
    "    # compute average loss per character\n",
    "    total_loss /= total_chars\n",
    "    \n",
    "    # set the model back to training mode in case we need gradients later\n",
    "    model.train()\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.824993203126884\n",
      "2.58618413380023\n",
      "2.51347909897662\n",
      "2.475872794491589\n",
      "2.4512298953839267\n",
      "2.432051269105194\n",
      "2.4165737255937305\n",
      "2.403743439609211\n",
      "2.3930218885134593\n",
      "2.383809648055891\n"
     ]
    }
   ],
   "source": [
    "model = RNNLM(vocab_size)\n",
    "train_model(model, Xtrain, Ytrain, Xval, Yval, id_to_char, max_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    " \n",
    "def random_pick(some_list, probabilities):\n",
    "    x = random.uniform(0,1)\n",
    "    cumulative_probability = 0.0\n",
    "    for item, item_probability in zip(some_list, probabilities):\n",
    "        cumulative_probability += item_probability\n",
    "        if  x < cumulative_probability:\n",
    "            break\n",
    "    return item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_pick([1,2,3], [0.2,0.75,0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_string(model, id_to_char, max_len=20, sample=True):\n",
    "    \"\"\"Generate a name using the model. The generation process should finish once\n",
    "    the end token is seen. We either sample from the model, where the next token is\n",
    "    chosen randomly according to the categorical probability distribution produced by softmax,\n",
    "    or we use argmax decoding where the most likely token is chosen at every generation step.\n",
    "\n",
    "    Args:\n",
    "        model (RNNLM): The trained RNNLM model.\n",
    "        id_to_char (dict(int, str)): A mapping from token ids to token strings.\n",
    "        max_len (int, optional): The maximum length of the output senquence. Defaults to 20.\n",
    "        sample (bool, optional): If True then generate samples. If False then use argmax decoding. \n",
    "            Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated name as a string.\n",
    "    \"\"\"\n",
    "    # put the model into eval mode because we don't need gradients\n",
    "    model.eval()\n",
    "\n",
    "    # setup the initial input to the network\n",
    "    # we will use a batch size of one for generation\n",
    "    h = torch.zeros((1,1,model.gru_size), dtype=torch.float) # h0 is all zeros\n",
    "    x = torch.ones((1, 1), dtype=torch.long) # x is the <bos> token id which = 1\n",
    "    out_str = \"\"\n",
    "    # generate the sequence step by step\n",
    "    for i in range(max_len):\n",
    "\n",
    "        # TODO: implement the generation loop of the RNNLM model\n",
    "        #       this should generate a name from the model\n",
    "        #       using either sampling or argmax decoding \n",
    "        if sample==True:\n",
    "            if out_str==\"\":\n",
    "                out,h_out = model(x,h)\n",
    "                probability = torch.nn.functional.softmax(out[0],dim=1)[0] \n",
    "                character_random = random_pick(vocab, probability)\n",
    "                index_char = char_to_id[character_random]\n",
    "                out_str+=character_random\n",
    "            \n",
    "            while len(out_str)<=20:\n",
    "                predicted_x =torch.tensor(np.array([[char_to_id[out_str[-1]]]]), dtype=torch.long)\n",
    "                index_char = char_to_id[character_random]\n",
    "                out,h_out = model(predicted_x,h)\n",
    "                p = torch.nn.functional.softmax(out[0],dim=1)[0] \n",
    "                character_random = random_pick(vocab, p)\n",
    "                out_str+=character_random\n",
    "            \n",
    "        if sample==False:\n",
    "            if out_str==\"\":\n",
    "                out,h_out = model(x,h)\n",
    "                probability = torch.nn.functional.softmax(out[0],dim=1)[0]\n",
    "                max_index = np.argmax(list(probability))\n",
    "                max_char = id_to_char[max_index]\n",
    "                out_str+=max_char\n",
    "            \n",
    "            while len(out_str)<=20:\n",
    "                #predicted_x = torch.tensor(np.array([[char_to_id[out_str[-1]]]]), dtype=torch.long)\n",
    "                x = x.fill_(max_index)\n",
    "                print('x',x)\n",
    "                out,h_out = model(x,h)\n",
    "                p = torch.nn.functional.softmax(out[0],dim=1)[0] \n",
    "                max_index = np.argmax(list(p))\n",
    "                max_char = id_to_char[max_index]\n",
    "                out_str+=max_char\n",
    "\n",
    "                \n",
    "\n",
    "    # set the model back to training mode in case we need gradients later\n",
    "    model.train()\n",
    "\n",
    "    return out_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_string(model, id_to_char, max_len=20, sample=True):\n",
    "    \"\"\"Generate a name using the model. The generation process should finish once\n",
    "    the end token is seen. We either sample from the model, where the next token is\n",
    "    chosen randomly according to the categorical probability distribution produced by softmax,\n",
    "    or we use argmax decoding where the most likely token is chosen at every generation step.\n",
    "\n",
    "    Args:\n",
    "        model (RNNLM): The trained RNNLM model.\n",
    "        id_to_char (dict(int, str)): A mapping from token ids to token strings.\n",
    "        max_len (int, optional): The maximum length of the output senquence. Defaults to 20.\n",
    "        sample (bool, optional): If True then generate samples. If False then use argmax decoding. \n",
    "            Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated name as a string.\n",
    "    \"\"\"\n",
    "    # put the model into eval mode because we don't need gradients\n",
    "    model.eval()\n",
    "\n",
    "    # setup the initial input to the network\n",
    "    # we will use a batch size of one for generation\n",
    "    h = torch.zeros((1,1,model.gru_size), dtype=torch.float) # h0 is all zeros\n",
    "    x = torch.ones((1, 1), dtype=torch.long) # x is the <bos> token id which = 1\n",
    "    out_str = \"\"\n",
    "    # generate the sequence step by step\n",
    "    for i in range(max_len):\n",
    "        out,h = model(x,h)\n",
    "        probability = torch.nn.functional.softmax(out[0],dim=1)[0]\n",
    "        predicted_X = random_pick(list(id_to_char.keys()), probability)  if sample else np.argmax(list(probability))\n",
    "        char_predict = id_to_char[predicted_X]\n",
    "        x = x.fill_(predicted_X)\n",
    "        if char_predict=='.':\n",
    "            break\n",
    "        out_str+=char_predict\n",
    "\n",
    "    # set the model back to training mode in case we need gradients later\n",
    "    model.train()\n",
    "\n",
    "    return out_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden size (1, 1, 32), got [1, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-71d6159816ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Argmax: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_to_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-230-e91ffca3ff96>\u001b[0m in \u001b[0;36mgen_string\u001b[0;34m(model, id_to_char, max_len, sample)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# generate the sequence step by step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mprobability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mpredicted_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_pick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_to_char\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mif\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-98a51e7595e4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h_last)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# TODO: implement this function which does the forward pass of the RNNLM network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m#out = self.linear(output[:,-1,:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    194\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden size (1, 1, 32), got [1, 32]"
     ]
    }
   ],
   "source": [
    "print(\"Argmax: \", gen_string(model, id_to_char, sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random:\n",
      "SGIscCd6<bos>RiN95LCpUJh\n",
      "M D AwickameMepcKKAK\n",
      "N3wOphulenYy NikeyFl\n",
      "ImiehulicAFwmpWadAJy\n",
      "Awkergakaxz\n",
      "GbexFycGhanilbetyngF\n",
      "JoFedu61\n",
      "CaByAthvihaxirx\n",
      "K2Vt JAFuNacKencKutr\n",
      "WAs Kmy TNkakiJidoll\n"
     ]
    }
   ],
   "source": [
    "print(\"Random:\")\n",
    "for i in range(10):\n",
    "    gstr = gen_string(model, id_to_char)\n",
    "    print(gstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,h_out = model(x,h)\n",
    "probability = torch.nn.functional.softmax(out[0],dim=1)[0]\n",
    "max_index = np.argmax(list(prpbability))\n",
    "max_char = id_to_char[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_str=\"\"\n",
    "out_str+=max_char\n",
    "index_array = np.array([[max_index]])\n",
    "x = torch.tensor(index_array, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[38]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out,h_out = model(x,h)\n",
    "probability = torch.nn.functional.softmax(out[0],dim=1)[0] \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0046, 0.0039, 0.0274, 0.0398, 0.0073, 0.0166, 0.0031, 0.0658, 0.0263,\n",
       "        0.0113, 0.1050, 0.0276, 0.0107, 0.0144, 0.0019, 0.0403, 0.0260, 0.1395,\n",
       "        0.0109, 0.0195, 0.0122, 0.0255, 0.0208, 0.0338, 0.0149, 0.0202, 0.0025,\n",
       "        0.0201, 0.0157, 0.0030, 0.0017, 0.0030, 0.0029, 0.0050, 0.0044, 0.0048,\n",
       "        0.0048, 0.0070, 0.0034, 0.0028, 0.0038, 0.0032, 0.0053, 0.0032, 0.0032,\n",
       "        0.0053, 0.0044, 0.0032, 0.0040, 0.0074, 0.0035, 0.0027, 0.0073, 0.0057,\n",
       "        0.0048, 0.0121, 0.0076, 0.0073, 0.0087, 0.0082, 0.0061, 0.0050, 0.0102,\n",
       "        0.0090, 0.0122, 0.0366], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_char[np.argmax(list(probability))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prpbability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.zeros((1,1,model.gru_size), dtype=torch.float) # h0 is all zeros\n",
    "x = torch.ones((1, 1), dtype=torch.long) # x is the <bos> token id which = 1\n",
    "out,h_out = model(x,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 66])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prpbability = torch.nn.functional.softmax(out[0],dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3214e-04, 2.0970e-04, 3.6157e-04, 2.1848e-03, 3.9716e-04, 7.0805e-04,\n",
       "        2.7473e-03, 8.4021e-04, 1.0081e-03, 5.4722e-04, 5.1366e-04, 1.2428e-03,\n",
       "        5.5267e-04, 6.0660e-04, 6.6937e-04, 4.7224e-04, 1.3742e-03, 1.2534e-03,\n",
       "        5.5245e-04, 4.6272e-04, 1.6006e-03, 9.1939e-04, 9.7934e-04, 5.6364e-04,\n",
       "        9.7742e-04, 2.8476e-04, 2.8139e-04, 9.2276e-04, 4.3837e-04, 5.6948e-02,\n",
       "        6.9449e-02, 5.4001e-02, 7.9029e-02, 3.1267e-02, 1.9711e-02, 3.9416e-02,\n",
       "        2.3743e-02, 6.0535e-03, 1.3475e-01, 3.6693e-02, 3.2935e-02, 8.9798e-02,\n",
       "        1.8913e-02, 8.1329e-03, 3.7928e-02, 1.0127e-03, 7.5102e-02, 5.9173e-02,\n",
       "        5.3103e-02, 2.4374e-03, 1.1766e-02, 1.8786e-02, 5.5004e-04, 7.5721e-03,\n",
       "        3.5901e-03, 2.9004e-04, 2.8214e-04, 3.1374e-04, 2.3527e-04, 1.9841e-04,\n",
       "        1.8452e-04, 1.5880e-04, 1.3005e-04, 2.0600e-04, 3.0708e-04, 1.9284e-03],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prpbability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_pick(vocab, prpbability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'J'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(list(prpbability))\n",
    "id_to_char[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros((1, 1), dtype=torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array = np.array([[0]])\n",
    "y = torch.tensor(y_array, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y==x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for epoch in range(1, 60):\n",
    "    model.fit(x, y, batch_size=128)\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = x[start_index: start_index + maxlen]\n",
    "    for i in range(400): #generates 400 length string\n",
    "        preds = model.predict(generated_text)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = chars[next_index]\n",
    "        generated_text += next_char\n",
    "        generated_text = generated_text[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X = Xtrain[:32]\n",
    "batch_Y = Ytrain[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, h = model(batch_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.nn.functional.softmax(out,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30,  7,  5,  2,  0,  0],\n",
       "       [36,  3, 16, 16,  3, 10],\n",
       "       [47, 11, 19, 11,  2,  0]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs_to_ids(['Bec.', 'Hannah.', 'Siqi.'], char_to_id, max_len=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqs_to_ids(seqs, char_to_id, max_len=20):\n",
    "    \"\"\"Takes a list of names and turns them into a list of tokens ids.\n",
    "    Responsible for padding sequences shorter than max_len with 0 so that all sequences are max_len.\n",
    "    Also truncates names that are longer than max_len.\n",
    "    Should also skip empty sequences if there are any.\n",
    "\n",
    "    Args:\n",
    "        seqs (list(str)): A list of names as strings.\n",
    "        char_to_id (dict(str : int)): The mapping for characters to token ids\n",
    "        max_len (int, optional): The maximum length of the ouput sequence. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "        np.array: the names represented using token ids as 2d numpy array, \n",
    "            where each row corresponds to a name. The size of the array should be N * max_len\n",
    "            where N is the number of non-empty sequences input. Padded with zeros if needed.\n",
    "    \"\"\"\n",
    "    all_seqs = []\n",
    "    # TODO: implement this function to turn a list of names into a 2d padded array of token ids\n",
    "    for name in seqs:\n",
    "        truncated = name[:max_len]\n",
    "        \n",
    "        name_sequence = [char_to_id[c] for c in truncated]\n",
    "        if (len(name_sequence))<max_len:\n",
    "            name_sequence=name_sequence+(max_len-len(name_sequence))*[0]\n",
    "        all_seqs.append(name_sequence)    \n",
    "    return np.array(all_seqs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
